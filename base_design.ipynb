{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# pull llama2\n",
    "ollama.pull(\"mistral\")\n",
    "# ollama.pull(\"mixtral\")  # mistral is the default model, use different.\n",
    "ollama.pull(\"codellama\") # TODO: maybe use larger model, specific model for math.\n",
    "# maybe use https://ollama.com/library/wizard-math for math-related stuff in parallel with codellama (can use 34b)\n",
    "# or maybe use https://ollama.com/solobsd/llemma-7b for math-related stuff in parallel with codellama (only can use 7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kg_data = {\n",
    "    \"Concept\": [\"derivative\", \"integral\", \"plot\"],\n",
    "    \"CodePattern\": [\n",
    "        \"Use the differentiation operator in sympy.\",\n",
    "        \"Use the integration operator in sympy.\",\n",
    "        \"Use matplotlib to plot the function.\",\n",
    "    ],\n",
    "}\n",
    "kg = pd.DataFrame(kg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: maybe use an llm here?\n",
    "def lookup_code_pattern(query):\n",
    "    keywords = [\"derivative\", \"integral\", \"plot\"]\n",
    "    for keyword in keywords:\n",
    "        if keyword in query.lower():\n",
    "            code_pattern = kg[kg[\"Concept\"] == keyword][\"CodePattern\"].iloc[0]\n",
    "    return keyword, code_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, call this knowledge to ollama's codellama solution.\n",
    "# todo: use instruct version instead of 7b-base\n",
    "def generate_code_llm(query):\n",
    "    concept, code_pattern = lookup_code_pattern(query)\n",
    "    query_appended = query + \"Here's a hint:\" + code_pattern\n",
    "\n",
    "    code_query = f\"You are an expert programmer that writes simple, concise code in Python. {query_appended} ONLY the code is needed,Absolutely NO explanation. Use SymPy.\"\n",
    "\n",
    "    print(\"query sent to codellama: \", code_query)\n",
    "    response = ollama.chat(\n",
    "        \"codellama\", messages=[{\"role\": \"system\", \"content\": code_query}]\n",
    "    )\n",
    "    resp = response[\"message\"][\"content\"].replace(\"```\", \"\")\n",
    "    print(\"response from codellama:\\n\", resp)\n",
    "    attempt = 5\n",
    "    while attempt > 0:\n",
    "        try:\n",
    "            code_result = exec(resp)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == 1:\n",
    "                continue\n",
    "            error = f\"Error in code execution: {e}\"\n",
    "            code_query += f\"\\nYou gave this response.\\n{resp}\\nHowever, execution of the response resulted in an error.\\n{error}\\nPlease correct the error and try again. ONLY the corrected code is needed,Absolutely NO explanation. Use SymPy.\"\n",
    "            print(\"query sent to codellama: \", code_query)\n",
    "            response = ollama.chat(\n",
    "                \"codellama\", messages=[{\"role\": \"system\", \"content\": code_query}]\n",
    "            )\n",
    "            resp = response[\"message\"][\"content\"].replace(\"```\", \"\")\n",
    "            print(\"response from codellama:\\n\", resp)\n",
    "            attempt -= 1\n",
    "\n",
    "    # todo: let answer llm know if the code generated by code LLM was executable\n",
    "    print(\"code generated by codellama: \", code_result)\n",
    "    return code_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_response(query):\n",
    "    concept, context_info = lookup_code_pattern(query)\n",
    "    # code result\n",
    "    code_result = generate_code_llm(query)\n",
    "    if concept:\n",
    "        response = ollama.chat(\n",
    "            model=\"mistral\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Answer the question {query} using the answer {code_result}. Just provide the answer.\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Compute the derivative of x^2 with respect to x.\"\n",
    "print(generate_llm_response(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Compute the integral of x^2 with respect to x.\"\n",
    "print(generate_llm_response(query2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
